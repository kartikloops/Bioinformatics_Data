{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1) SCRAPE LATEST TOPICS USING SELENIUM\n",
    "# ------------------------------------------\n",
    "driver = webdriver.Chrome()  # or webdriver.Firefox(), etc.\n",
    "driver.get(\"https://community.seqera.io/latest\")  # The page we'll scrape\n",
    "\n",
    "#Option 1 : https://opennms.discourse.group/latest\n",
    "#Option 2 : https://help.galaxyproject.org/latest\n",
    "#Option 3 : https://cwl.discourse.group/latest\n",
    "\n",
    "\n",
    "# Scroll to load all topics\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Wait a bit for new content to load\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Get the complete page source after scrolling\n",
    "page_source = driver.page_source\n",
    "driver.quit()\n",
    "\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "tbody = soup.find(\"tbody\", {\"class\": \"topic-list-body\"})\n",
    "\n",
    "# Safety check: if tbody not found, handle gracefully\n",
    "if not tbody:\n",
    "    print(\"No topics found. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "rows = tbody.find_all(\"tr\")\n",
    "\n",
    "# Prepare lists (or skip direct lists if you'll store data in dictionaries)\n",
    "titles = []\n",
    "hrefs = []\n",
    "replies_list = []\n",
    "\n",
    "\n",
    "for row in rows:\n",
    "    # 1) Title + href\n",
    "    link_tag = row.find(\"a\", class_=\"title raw-link raw-topic-link\")\n",
    "    if link_tag:\n",
    "        title_text = link_tag.get_text(strip=True)\n",
    "        title_href = link_tag.get(\"href\")\n",
    "        titles.append(title_text)\n",
    "        hrefs.append(title_href)\n",
    "    else:\n",
    "        titles.append(None)\n",
    "        hrefs.append(None)\n",
    "\n",
    "    # 2) Number of replies\n",
    "    button_tag = row.find(\"button\", class_=\"btn-link posts-map badge-posts\")\n",
    "    if button_tag:\n",
    "        span_tag = button_tag.find(\"span\", class_=\"number\")\n",
    "        replies_list.append(span_tag.get_text(strip=True) if span_tag else None)\n",
    "    else:\n",
    "        replies_list.append(None)\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2) BUILD A LIST OF TOPIC DICTIONARIES\n",
    "# ------------------------------------------\n",
    "topics_data = []\n",
    "for i in range(len(titles)):\n",
    "    title = titles[i]\n",
    "    link = hrefs[i]\n",
    "    replies = replies_list[i]\n",
    "\n",
    "\n",
    "    if link is None:\n",
    "        continue  # Skip if no link found\n",
    "\n",
    "    # The link is something like: /t/slug-title/1578\n",
    "    # We can extract the numeric ID from the tail of the URL\n",
    "    # e.g. 1578 from '/t/some-topic/1578'\n",
    "    post_id = None\n",
    "    match = re.search(r'/(\\d+)$', link)\n",
    "    if match:\n",
    "        post_id = match.group(1)\n",
    "\n",
    "    # Store partial data now; weâ€™ll add the 'body' in the next step\n",
    "    topic_info = {\n",
    "        \"title\": title,\n",
    "        \"relative_link\": link,\n",
    "        \"post_id\": post_id,\n",
    "        \"replies\": replies,\n",
    "        \"created_date\": created,\n",
    "        \"latest_date\": latest\n",
    "    }\n",
    "    topics_data.append(topic_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 3) SAVE RESULTS TO JSON\n",
    "# ------------------------------------------\n",
    "output_filename = \"seqera.json\"\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(topics_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"\\nDone! Scraped {len(topics_data)} topics. See '{output_filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to extract the body of the post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import time\n",
    "# import re\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # 1) Load the JSON data you already have\n",
    "# with open(\"forum_half_body.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     topics_data = json.load(f)\n",
    "\n",
    "# # 2) Filter out the topics that you still need to scrape\n",
    "# #    For example, skip those that already have a body or whose post_id <= 1581\n",
    "# topics_to_scrape = []\n",
    "# for t in topics_data:\n",
    "#     # If 'body' is None or empty, and post_id is greater than 1581\n",
    "#     # (Adjust the condition as you prefer.)\n",
    "#     if (not t.get(\"body\")) and t.get(\"post_id\"):\n",
    "#         try:\n",
    "#             if int(t[\"post_id\"]) > 1581:\n",
    "#                 topics_to_scrape.append(t)\n",
    "#         except ValueError:\n",
    "#             pass  # handle if post_id is not an integer string\n",
    "\n",
    "# print(f\"Total topics loaded: {len(topics_data)}\")\n",
    "# print(f\"Topics to scrape now: {len(topics_to_scrape)}\")\n",
    "\n",
    "# # 3) Use Selenium to scrape only the remaining topics\n",
    "# driver = webdriver.Chrome()\n",
    "# driver.implicitly_wait(30)\n",
    "\n",
    "# base_url = \"https://community.seqera.io\"\n",
    "\n",
    "# for topic in topics_to_scrape:\n",
    "#     full_url = base_url + topic[\"relative_link\"]\n",
    "#     driver.get(full_url)\n",
    "#     time.sleep(2)  # Adjust or use explicit waits\n",
    "\n",
    "#     page_source_post = driver.page_source\n",
    "#     soup_post = BeautifulSoup(page_source_post, \"html.parser\")\n",
    "#     cooked_div = soup_post.find('div', class_='cooked')\n",
    "\n",
    "#     if cooked_div:\n",
    "#         body_text = cooked_div.get_text(separator=\"\\n\", strip=True)\n",
    "#         topic[\"body\"] = body_text\n",
    "#         print(f\"Scraped body for post_id={topic['post_id']}\")\n",
    "#     else:\n",
    "#         topic[\"body\"] = None\n",
    "#         print(f\"No 'cooked' div found for post_id={topic['post_id']}\")\n",
    "\n",
    "# # Done scraping, close driver\n",
    "# driver.quit()\n",
    "\n",
    "# # 4) Save updated data back into the JSON\n",
    "# with open(\"seqera_topics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(topics_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# print(\"Partial scraping complete. JSON updated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
